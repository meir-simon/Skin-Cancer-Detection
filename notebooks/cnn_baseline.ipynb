{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32a5349-dba2-4e9f-822c-594960d78aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26edfea9-2eef-4231-9d02-f2a58b831b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a69660-73fe-45db-a02f-d325b83f5dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_KAGGLE:\n",
    "    ROOT_DIR = \"/kaggle/input/isic-2024-challenge\"\n",
    "    SCORE_PATH = \"/kaggle/input/isic-2024-challenge-selfclean-scores/ISIC_2024_Challenge_SelfClean_Scores.csv\"\n",
    "else:\n",
    "    ROOT_DIR = \"../data/ISIC_2024\"\n",
    "    SCORE_PATH = \"../data/ISIC_2024/ISIC_2024_Challenge_SelfClean_Scores.csv\"\n",
    "TRAIN_DIR = \"train-image/image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e229ce5f-1174-49b2-9607-a8ae8f9c9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, transform=None):\n",
    "        self.df = df\n",
    "        self.file_names = df[\"file_path\"].values\n",
    "        self.targets = df[\"target\"].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        img_path = self.file_names[index]\n",
    "        target = self.targets[index]\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img = img.convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, int(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e937749f-432c-4882-a14b-d1cb5377c9d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/ISIC_2024/train-metadata.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m Path(ROOT_DIR) \u001b[38;5;241m/\u001b[39m TRAIN_DIR\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mROOT_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train-metadata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(glob\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/*.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misic_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m image_id: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/kaggle/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/ISIC_2024/train-metadata.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dir = Path(ROOT_DIR) / TRAIN_DIR\n",
    "df = pd.read_csv(f\"{ROOT_DIR}/train-metadata.csv\")\n",
    "\n",
    "train_images = sorted(glob.glob(f\"{train_dir}/*.jpg\"))\n",
    "df[\"file_path\"] = df[\"isic_id\"].apply(\n",
    "    lambda image_id: f\"{train_dir}/{image_id}.jpg\"\n",
    ")\n",
    "df = df[df[\"file_path\"].isin(train_images)].reset_index(drop=True)\n",
    "df['target_name'] = df['target'].replace(0, 'Benign').replace(1, 'Malignant')\n",
    "\n",
    "dataset = ISICDataset(df=df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ba7bc4f-ee21-4590-8202-b7b75d31be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=224\n",
    "BATCH_SIZE=32\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_model(model_name=\"efficientnet_v2_m\"):\n",
    "    if model_name == \"mobilenet_v3_small\":\n",
    "    # mobile net\n",
    "        model = models.mobilenet_v3_small()\n",
    "        model.classifier[3] = torch.nn.Linear(model.classifier[3].in_features, 1)\n",
    "    if model_name == \"efficientnet_v2_m\":\n",
    "        model = models.efficientnet_v2_m(weights=None)\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 1)\n",
    "    if model_name == \"efficientnet_b0\":\n",
    "        model = models.efficientnet_b0(weights=None)\n",
    "        model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 1)\n",
    "\n",
    "    if model_name == \"vgg16\":\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 1)        \n",
    "    \n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = BCEWithLogitsLoss()\n",
    "    return model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f03dc390-acee-40fe-a7f6-535b198b93bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a339cfdb-349c-4eef-a5d8-5a9de4ec3722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mean and std of train data\n",
    "train_dataset = ISICDataset(train_data,\n",
    "                          transform=transforms.Compose([\n",
    "                                transforms.Resize((IMG_SIZE, IMG_SIZE)),            \n",
    "                                transforms.ToTensor(),\n",
    "                        ]))    \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)    \n",
    "\n",
    "# mean = 0.0\n",
    "# for images, _,_ in train_loader:\n",
    "#     batch_samples = images.size(0) # batch size (the last batch can have smaller size!)        \n",
    "#     images = images.view(batch_samples, images.size(1), -1) \n",
    "#     mean += images.mean(2).sum(0)  \n",
    "# train_data_mean = mean / len(train_loader.dataset)\n",
    "\n",
    "# var = 0.0\n",
    "# for images, _,_ in train_loader:\n",
    "#     batch_samples = images.size(0)\n",
    "#     images = images.view(batch_samples, images.size(1), -1)\n",
    "#     var += ((images - train_data_mean.unsqueeze(1))**2).sum([0,2])\n",
    "# train_data_std = torch.sqrt(var / (len(train_loader.dataset)*IMG_SIZE*IMG_SIZE))\n",
    "# print(f\"mean {train_data_mean}, std {train_data_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce17aa0-3925-450c-af57-c0ab7b59b9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_probs = []        \n",
    "\n",
    "    model.train()\n",
    "    for input,_, targets in train_loader:        \n",
    "        input = input.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "\n",
    "        targets = targets.unsqueeze(1) # make the target [batch, 1]\n",
    "        targets = targets.float() # BCEWithLogitsLoss requires targets as float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = criterion(output, targets)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(output).cpu().detach().numpy()\n",
    "\n",
    "        all_targets.extend(targets.cpu().detach().numpy().flatten())\n",
    "        all_probs.extend(probs.flatten())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    pauc = compute_pauc(np.array(all_targets), np.array(all_probs))\n",
    "    return total_loss, pauc\n",
    "\n",
    "def val(model, val_loader, criterion):\n",
    "    total_loss= 0\n",
    "    all_targets = []\n",
    "    all_probs = []        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, _, targets in val_loader:\n",
    "            input = input.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "\n",
    "            targets = targets.unsqueeze(1) # make the target [batch, 1]\n",
    "            targets = targets.float() # BCEWithLogitsLoss requires targets as float()\n",
    "\n",
    "            output = model(input)\n",
    "            val_loss = criterion(output, targets)\n",
    "            total_loss +=  val_loss.item()\n",
    "\n",
    "            sigmoid = torch.nn.Sigmoid()\n",
    "            probs = sigmoid(output).cpu().detach().numpy()\n",
    "            \n",
    "            all_targets.extend(targets.cpu().detach().numpy().flatten())\n",
    "            all_probs.extend(probs.flatten())           \n",
    "    \n",
    "    pauc = compute_pauc(np.array(all_targets), np.array(all_probs))\n",
    "    return total_loss, pauc, all_probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a748f91a-10ad-42c3-a585-e064dbab2d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_ID    = 1\n",
    "MODEL_NAME = \"efficientnet_v2_m\"\n",
    "NUM_EPOCHS = 30\n",
    "# BATCH_SIZE = 32\n",
    "NOTE=\"with_external_db\"\n",
    "EXP_NAME = \"{:03}_{}_{}_{}_{}\".format(EXP_ID, MODEL_NAME, NUM_EPOCHS, BATCH_SIZE, NOTE)  # you can name your experiment whatever you like\n",
    "SAVE_PATH = \"/kaggle/working\"\n",
    "\n",
    "# train_trans = transforms.Compose([    \n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),       \n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=train_data_mean, std=train_data_std),\n",
    "# ])\n",
    "# val_trans =  transforms.Compose([    \n",
    "#     transforms.Resize((IMG_SIZE, IMG_SIZE)),  \n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=train_data_mean, std=train_data_std),\n",
    "# ])\n",
    "\n",
    "trn_dataset = ISICDataset(train_data,transform=train_trans)\n",
    "val_dataset = ISICDataset(val_data,transform=val_trans)\n",
    "\n",
    "train_loader = DataLoader(trn_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"{len(trn_dataset)}, {len(val_dataset)}\")\n",
    "model, optimizer, criterion = load_model(\"efficientnet_v2_m\")\n",
    "\n",
    "best_val_loss, best_val_pauc = 100, 0\n",
    "\n",
    "## training loop\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_pauc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_pauc, _ = val(model, val_loader, criterion)        \n",
    "    \n",
    "    if val_pauc > best_val_pauc:\n",
    "        best_val_pauc = val_pauc\n",
    "        os.makedirs(f\"{SAVE_PATH}/{EXP_NAME}\", exist_ok=True)            \n",
    "        torch.save(model.state_dict(),f\"{SAVE_PATH}/{EXP_NAME}/best_all.pth\")\n",
    "        print(f\"Epoch {epoch}, train_loss {train_loss:.4f}, train_pauc {train_pauc:.2f}, val_loss {val_loss:.4f}, val_pauc {val_pauc:.2f} --> Best val_pauc {val_pauc:.2f} at epoch {epoch}\")    \n",
    "\n",
    "    else:        \n",
    "        print(f\"Epoch {epoch}, train_loss {train_loss:.4f}, train_pauc {train_pauc:.2f}, val_loss {val_loss:.4f}, val_pauc {val_pauc:.2f}\") \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
