{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#maybe you should run this first\n",
        "# pip install torch torchvision\n",
        "# pip install pillow"
      ],
      "metadata": {
        "id": "YPp3nuflfiM8"
      },
      "id": "YPp3nuflfiM8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import glob\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from sklearn.model_selection import GroupShuffleSplit\n"
      ],
      "metadata": {
        "id": "O04ku4yNvMZk"
      },
      "id": "O04ku4yNvMZk",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IN_KAGGLE = \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ\n",
        "IN_COLAB = \"COLAB_GPU\" in os.environ"
      ],
      "metadata": {
        "id": "8H-BJ2yzvBDU"
      },
      "id": "8H-BJ2yzvBDU",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d32a5349-dba2-4e9f-822c-594960d78aef",
      "metadata": {
        "id": "d32a5349-dba2-4e9f-822c-594960d78aef"
      },
      "outputs": [],
      "source": [
        "#before you run this cell make sure that your kaggle key and your kaggle username are saved at colab/secrets\n",
        "if IN_COLAB:\n",
        "  from google.colab import userdata\n",
        "  #download the data to colab\n",
        "  os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "  os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "  ! kaggle competitions download isic-2024-challenge\n",
        "  ! unzip isic-2024-challenge.zip\n",
        "  IMG_PATH = \"/content/train-image/image\"\n",
        "  CSV_PATH = \"/content/train-metadata.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_KAGGLE:\n",
        "  IMG_PATH = \"/kaggle/input/isic-2024-challenge/train-image/image\"\n",
        "  CSV_PATH = \"/kaggle/input/isic-2024-challenge/train-metadata.csv\"\n"
      ],
      "metadata": {
        "id": "TUTagldjhBdQ"
      },
      "id": "TUTagldjhBdQ",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_file_path(image_id):\n",
        "    return f\"{IMG_PATH}/{image_id}.jpg\""
      ],
      "metadata": {
        "id": "zDQlU1eLyFlY"
      },
      "id": "zDQlU1eLyFlY",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add the images path to the df\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "train_images = sorted(glob.glob(f\"{IMG_PATH}/*.jpg\"))# make list of all the imges that realy exist in the image folder\n",
        "df['file_path'] = df['isic_id'].apply(get_train_file_path)# add a \"file_path\" feature to each row\n",
        "df = df[ df[\"file_path\"].isin(train_images) ].reset_index(drop=True)# keep only the rows that their images in train_images\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "efrqDjS2yFSh",
        "collapsed": true
      },
      "id": "efrqDjS2yFSh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# consider to create a sample from the df, just to check that all the proces workes well\n",
        "# print(\"          df.shape, # of positive cases, # of patients\")\n",
        "# print(\"original>\", df.shape, df.target.sum())\n",
        "# df_positive = df[df[\"target\"] == 1].reset_index(drop=True)\n",
        "# df_negative = df[df[\"target\"] == 0].reset_index(drop=True)\n",
        "# small_df = pd.concat([df_positive, df_negative.iloc[:df_positive.shape[0]*20, :]])  # positive:negative = 1:20\n",
        "# print(\"filtered>\", small_df.shape, df.target.sum())"
      ],
      "metadata": {
        "id": "H1Hh7BW4PuRf"
      },
      "id": "H1Hh7BW4PuRf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_by_patients(train_data_frame, target_column='target', patient_column='patient_id', train_size=0.85, drop_columns=False):\n",
        "    '''\n",
        "    This function receives a data frame and splits by patients while maintaining the target ratio.\n",
        "    :param train_data_frame: Training data frame with the patient IDs and targets inside.\n",
        "    :param target_column: Name of the target column, 'target' is default.\n",
        "    :param patient_column: Name of the patient column, 'patient_id' is default.\n",
        "    :param train_size: Percentage of data to become the training set, 0.85 is default.\n",
        "    :param drop_columns: When True: target and patient columns are dropped.\n",
        "    :return: A tuple of 4: x_train, y_train, x_test, y_test.\n",
        "    '''\n",
        "    targets = train_data_frame[target_column]\n",
        "    patients = train_data_frame[patient_column]\n",
        "\n",
        "    if drop_columns:\n",
        "        train_data_frame.drop(columns=['target', 'patient_id'], inplace=True)\n",
        "\n",
        "    # Split the data by patients, while keeping the positive cases distributed properly\n",
        "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=42)\n",
        "    train_idx, test_idx = next(gss.split(train_data_frame, groups=patients, y=targets))\n",
        "    x_train, x_test = train_data_frame.iloc[train_idx], train_data_frame.iloc[test_idx]\n",
        "    y_train, y_test = [targets[i] for i in train_idx], [targets[i] for i in test_idx]\n",
        "\n",
        "    # Print split stats\n",
        "    original_train_size = train_data_frame.shape[0]\n",
        "    train_size = x_train.shape[0]\n",
        "    original_positive_cases = targets.sum()\n",
        "    train_positive_cases = sum(y_train)\n",
        "    print(f'Data split: {train_size * 100 / original_train_size}, {100 - (train_size * 100 / original_train_size)}')\n",
        "    print(f'Positives cases split: {train_positive_cases * 100 / original_positive_cases}, {100 - (train_positive_cases * 100 / original_positive_cases)}')\n",
        "\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "metadata": {
        "id": "XJ1ZJWD5YQa0"
      },
      "id": "XJ1ZJWD5YQa0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split to train and validation sets\n",
        "\n",
        "train_df,_,valid_df,__ =  split_by_patients(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xwp9sYhPYlN",
        "outputId": "c1d355fe-3438-4631-a25a-ed430e2bc477"
      },
      "id": "4xwp9sYhPYlN",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split: 84.60401088119204, 15.395989118807961\n",
            "Positives cases split: 85.49618320610686, 14.503816793893137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_auc(y_test,y_pred):\n",
        "    fpr, tpr,_= roc_curve(y_test, y_pred)\n",
        "    new_tpr = []\n",
        "    for num in tpr:\n",
        "        if num>=0.8:\n",
        "            new_tpr.append(0.8)\n",
        "        else:\n",
        "            new_tpr.append(num)\n",
        "    return auc(fpr,tpr)-auc(fpr,new_tpr)"
      ],
      "metadata": {
        "id": "7cPeFKCe-kAG"
      },
      "id": "7cPeFKCe-kAG",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e229ce5f-1174-49b2-9607-a8ae8f9c9221",
      "metadata": {
        "id": "e229ce5f-1174-49b2-9607-a8ae8f9c9221"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ISICDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame, transform=None): # the df is the csv train data\n",
        "        self.df = df\n",
        "        self.file_names = df[\"file_path\"].values #the df is already with a \"file path\" column\n",
        "        self.targets = df[\"target\"].values\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df) #return the number of rows\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        img_path = self.file_names[index]\n",
        "        target = self.targets[index]\n",
        "\n",
        "        img = Image.open(img_path)\n",
        "        img = img.convert(\"RGB\")\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, int(target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7ba7bc4f-ee21-4590-8202-b7b75d31be07",
      "metadata": {
        "id": "7ba7bc4f-ee21-4590-8202-b7b75d31be07"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE=224\n",
        "BATCH_SIZE=32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_model(model_name):\n",
        "    if model_name == \"vgg16\":\n",
        "        model = models.vgg16(pretrained=True)\n",
        "        #replece the classfier with liniar layer with one neuron with weights according to the features in the previese layer\n",
        "        model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 1)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    return model, optimizer, criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a339cfdb-349c-4eef-a5d8-5a9de4ec3722",
      "metadata": {
        "id": "a339cfdb-349c-4eef-a5d8-5a9de4ec3722"
      },
      "outputs": [],
      "source": [
        "#instatiata a dataset for train set\n",
        "train_dataset = ISICDataset(train_df,\n",
        "                          transform=transforms.Compose([\n",
        "                                transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                                transforms.ToTensor(),\n",
        "                        ]))\n",
        "#instatiata a dataset for valid set\n",
        "valid_dataset = ISICDataset(valid_df,\n",
        "                          transform=transforms.Compose([\n",
        "                                transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "                                transforms.ToTensor(),\n",
        "                        ]))\n",
        "#instatiata a dataloader for train set\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#instatiata a dataloader for valid set\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "cce17aa0-3925-450c-af57-c0ab7b59b9e9",
      "metadata": {
        "id": "cce17aa0-3925-450c-af57-c0ab7b59b9e9"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, optimizer, criterion):\n",
        "  \"\"\"\n",
        "  train the model with baches of images given by the train_loader,return the total loss and the auc on the train data\n",
        "  \"\"\"\n",
        "  total_loss = 0\n",
        "  all_targets = []\n",
        "  all_probs = []\n",
        "  count_baches = 0\n",
        "  model.train()\n",
        "  for input, targets in train_loader:#when iterating over the data loader, we get bach of pics - tensor(32,3,224,224), and vector of the lables(32)\n",
        "      input = input.to(DEVICE)\n",
        "      targets = targets.to(DEVICE)\n",
        "\n",
        "      targets = targets.unsqueeze(1) #add dim to the targets tensor, it will look like [[1],[0]..]\n",
        "      targets = targets.float() # BCEWithLogitsLoss requires targets as float()\n",
        "      optimizer.zero_grad()\n",
        "      output = model(input)\n",
        "      loss = criterion(output, targets)# calculate the loss on the bach, return [scalar]\n",
        "      total_loss += loss.item()#add the skalar to the total lose\n",
        "\n",
        "      sigmoid = torch.nn.Sigmoid()\n",
        "      probs = sigmoid(output).cpu().detach().numpy()#create a np array with probabilities predicted\n",
        "\n",
        "      all_targets.extend(targets.cpu().detach().numpy().flatten())\n",
        "      all_probs.extend(probs.flatten())\n",
        "\n",
        "      loss.backward() #calculate dloss/dx for every parameter x\n",
        "      optimizer.step() # adjust the parameters accordingly\n",
        "\n",
        "      #count the baches to see where we are\n",
        "      count_baches+=1\n",
        "      for i in range(1,11):\n",
        "        if count_baches == i*1000:\n",
        "          print(f\"{i*100} baches already passed to the model\")\n",
        "\n",
        "  auc = calculate_auc(np.array(all_targets),np.array(all_probs))\n",
        "  return total_loss, auc\n",
        "\n",
        "def val(model, val_loader, criterion):\n",
        "    total_loss= 0\n",
        "    all_targets = []\n",
        "    all_probs = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for input, targets in val_loader:\n",
        "            input = input.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            targets = targets.unsqueeze(1) # make the target [batch, 1]\n",
        "            targets = targets.float() # BCEWithLogitsLoss requires targets as float()\n",
        "\n",
        "            output = model(input)\n",
        "            val_loss = criterion(output, targets)\n",
        "            total_loss +=  val_loss.item()\n",
        "\n",
        "            sigmoid = torch.nn.Sigmoid()\n",
        "            probs = sigmoid(output).cpu().detach().numpy()\n",
        "\n",
        "            all_targets.extend(targets.cpu().detach().numpy().flatten())\n",
        "            all_probs.extend(probs.flatten())\n",
        "\n",
        "    auc = calculate_auc(np.array(all_targets),np.array(all_probs))\n",
        "    return total_loss, auc\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a748f91a-10ad-42c3-a585-e064dbab2d1c",
      "metadata": {
        "id": "a748f91a-10ad-42c3-a585-e064dbab2d1c"
      },
      "outputs": [],
      "source": [
        "EXP_ID    = 1\n",
        "MODEL_NAME = \"vgg16\"\n",
        "NUM_EPOCHS =7\n",
        "BATCH_SIZE = 32\n",
        "# NOTE=\"with_external_db\"\n",
        "EXP_NAME = \"{:03}_{}_{}_{}\".format(EXP_ID, MODEL_NAME, NUM_EPOCHS, BATCH_SIZE)  # you can name your experiment whatever you like\n",
        "SAVE_PATH = \"/kaggle/working\"\n",
        "\n",
        "model, optimizer, criterion = load_model(\"vgg16\")\n",
        "\n",
        "\n",
        "\n",
        "## training loop\n",
        "best_val_pauc = 0\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  print(f\"start epoch:{epoch}\")\n",
        "  train_loss, train_pauc = train(model, train_loader, optimizer, criterion)\n",
        "  print(f\"Epoch {epoch} has finished\")\n",
        "  print(f\"start to predict the validation set, the results  are:\")\n",
        "  valid_loss, valid_pauc = val(model, valid_loader, criterion)\n",
        "  #save the best model so far\n",
        "  if valid_pauc > best_val_pauc:\n",
        "      best_val_pauc = valid_pauc\n",
        "      os.makedirs(f\"{SAVE_PATH}/{EXP_NAME}\", exist_ok=True)\n",
        "      torch.save(model.state_dict(),f\"{SAVE_PATH}/{EXP_NAME}/best_all.pth\")\n",
        "      print(f\"Epoch {epoch}, train_loss {train_loss:.4f}, train_pauc {train_pauc}, valid_loss {valid_loss:.4f}, valid_pauc {val_pauc} --> Best valid_pauc {valid_pauc} at epoch {epoch}\")\n",
        "\n",
        "  else:\n",
        "      print(f\"Epoch {epoch}, train_loss {train_loss:.4f}, train_pauc {train_pauc:.2f}, val_loss {val_loss:.4f}, val_pauc {val_pauc:.2f}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}